{
 "input": {
   "workflow": {
      "1": {
        "inputs": {
          "url_or_path": "https://ai-cdn.quinn.live/orgs/org_2uRK5d5oMuss5hEdrotp22wqsTJ/assets/GARMENT_IMAGE/cmh1r1yui000701qlet8a12ug/image.jpeg?timestamp=1761122917635"
        },
        "class_type": "LoadImageFromUrlOrPath",
        "_meta": {
          "title": "Load Reference Image"
        }
      },
      "2": {
        "inputs": {
          "upscale_method": "lanczos",
          "megapixels": 2.0000000000000004,
          "image": [
            "1",
            0
          ]
        },
        "class_type": "ImageScaleToTotalPixels",
        "_meta": {
          "title": "Scale Image to Total Pixels"
        }
      },
      "11": {
        "inputs": {
          "url_or_path": "https://ai-cdn.quinn.live/orgs/org_2uRK5d5oMuss5hEdrotp22wqsTJ/assets/GENERATED_IMAGE/cmh1r3yxt0026ql01h6uvy64p/image.jpeg"
        },
        "class_type": "LoadImageFromUrlOrPath",
        "_meta": {
          "title": "Load Input Image"
        }
      },
      "13": {
        "inputs": {
          "unet_name": "flux1-kontext-dev.safetensors",
          "weight_dtype": "default"
        },
        "class_type": "UNETLoader",
        "_meta": {
          "title": "Load Diffusion Model"
        }
      },
      "14": {
        "inputs": {
          "clip_name1": "t5xxl_fp8_e4m3fn_scaled.safetensors",
          "clip_name2": "clip_l.safetensors",
          "type": "flux",
          "device": "default"
        },
        "class_type": "DualCLIPLoader",
        "_meta": {
          "title": "DualCLIPLoader"
        }
      },
      "15": {
        "inputs": {
          "vae_name": "ae.safetensors"
        },
        "class_type": "VAELoader",
        "_meta": {
          "title": "Load VAE"
        }
      },
      "19": {
        "inputs": {
          "noise_mask": true,
          "positive": [
            "23",
            0
          ],
          "negative": [
            "25",
            0
          ],
          "vae": [
            "15",
            0
          ],
          "pixels": [
            "28",
            0
          ],
          "mask": [
            "85",
            0
          ]
        },
        "class_type": "InpaintModelConditioning",
        "_meta": {
          "title": "InpaintModelConditioning"
        }
      },
      "20": {
        "inputs": {
          "seed": 1756917890548,
          "steps": 15,
          "cfg": 1,
          "sampler_name": "deis",
          "scheduler": "beta",
          "denoise": 0.3,
          "model": [
            "13",
            0
          ],
          "positive": [
            "19",
            0
          ],
          "negative": [
            "19",
            1
          ],
          "latent_image": [
            "19",
            2
          ]
        },
        "class_type": "KSampler",
        "_meta": {
          "title": "KSampler"
        }
      },
      "21": {
        "inputs": {
          "pixels": [
            "2",
            0
          ],
          "vae": [
            "15",
            0
          ]
        },
        "class_type": "VAEEncode",
        "_meta": {
          "title": "VAE Encode"
        }
      },
      "22": {
        "inputs": {
          "guidance": 2.5,
          "conditioning": [
            "24",
            0
          ]
        },
        "class_type": "FluxGuidance",
        "_meta": {
          "title": "FluxGuidance"
        }
      },
      "23": {
        "inputs": {
          "conditioning": [
            "22",
            0
          ],
          "latent": [
            "21",
            0
          ]
        },
        "class_type": "ReferenceLatent",
        "_meta": {
          "title": "ReferenceLatent"
        }
      },
      "24": {
        "inputs": {
          "text": "enhance this image",
          "clip": [
            "14",
            0
          ]
        },
        "class_type": "CLIPTextEncode",
        "_meta": {
          "title": "CLIP Text Encode (Positive Prompt)"
        }
      },
      "25": {
        "inputs": {
          "conditioning": [
            "24",
            0
          ]
        },
        "class_type": "ConditioningZeroOut",
        "_meta": {
          "title": "ConditioningZeroOut"
        }
      },
      "28": {
        "inputs": {
          "upscale_method": "lanczos",
          "megapixels": 2.0000000000000004,
          "image": [
            "82",
            0
          ]
        },
        "class_type": "ImageScaleToTotalPixels",
        "_meta": {
          "title": "Scale Image to Total Pixels"
        }
      },
      "44": {
        "inputs": {
          "samples": [
            "20",
            0
          ],
          "vae": [
            "15",
            0
          ]
        },
        "class_type": "VAEDecode",
        "_meta": {
          "title": "VAE Decode"
        }
      },
      "82": {
        "inputs": {
          "padding_left": 10,
          "padding_right": 10,
          "padding_top": 10,
          "padding_bottom": 10,
          "return_list": false,
          "image": [
            "11",
            0
          ],
          "mask": [
            "119",
            0
          ]
        },
        "class_type": "Bounded Image Crop with Mask",
        "_meta": {
          "title": "Bounded Image Crop with Mask"
        }
      },
      "83": {
        "inputs": {
          "image": [
            "84",
            0
          ],
          "image_bounds": [
            "82",
            1
          ]
        },
        "class_type": "Bounded Image Crop",
        "_meta": {
          "title": "Bounded Image Crop"
        }
      },
      "84": {
        "inputs": {
          "mask": [
            "119",
            0
          ]
        },
        "class_type": "MaskToImage",
        "_meta": {
          "title": "Convert Mask to Image"
        }
      },
      "85": {
        "inputs": {
          "channel": "red",
          "image": [
            "83",
            0
          ]
        },
        "class_type": "ImageToMask",
        "_meta": {
          "title": "Convert Image to Mask"
        }
      },
      "117": {
        "inputs": {
          "blend_factor": 1,
          "feathering": 2,
          "target": [
            "11",
            0
          ],
          "target_mask": [
            "119",
            0
          ],
          "target_bounds": [
            "82",
            1
          ],
          "source": [
            "44",
            0
          ]
        },
        "class_type": "Bounded Image Blend with Mask",
        "_meta": {
          "title": "Bounded Image Blend with Mask"
        }
      },
      "119": {
        "inputs": {
          "expand": -1,
          "tapered_corners": true,
          "mask": [
            "127",
            0
          ]
        },
        "class_type": "GrowMask",
        "_meta": {
          "title": "GrowMask"
        }
      },
      "120": {
        "inputs": {
          "url_or_path": "https://storage.googleapis.com/ai-cdn.quinn.live/masks/cmh1xh4ix00tbnt01oeltbfi3-subject-0/cmh1xh88g00jo01nt4z1w8uw0_garment_mask.png"
        },
        "class_type": "LoadImageFromUrlOrPath",
        "_meta": {
          "title": "Load Input Image Mask"
        }
      },
      "122": {
        "inputs": {
          "channel": "red",
          "image": [
            "120",
            0
          ]
        },
        "class_type": "ImageToMask",
        "_meta": {
          "title": "Convert Image to Mask"
        }
      },
      "127": {
        "inputs": {
          "masks": [
            "122",
            0
          ]
        },
        "class_type": "Mask Fill Holes",
        "_meta": {
          "title": "Mask Fill Holes"
        }
      },
      "139": {
        "inputs": {
          "filename_prefix": "test",
          "images": [
            "117",
            0
          ]
        },
        "class_type": "SaveImage",
        "_meta": {
          "title": "Save Image"
        }
      }
  },
  "upload": {
    "bucket": "ai-cdn.quinn.live",
    "key": "test/test.png"
  }
 }
}
